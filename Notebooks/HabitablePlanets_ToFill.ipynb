{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accompanying notebook for Habitable Planets selection problem (student version; to fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author: Viviana Acquaviva\n",
    "\n",
    "#License: BSD but really should be TBD - just be nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics \n",
    "from scipy import stats\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import neighbors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "font = {'size'   : 20}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('xtick', labelsize=20) \n",
    "matplotlib.rc('ytick', labelsize=20) \n",
    "matplotlib.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data from http://phl.upr.edu/projects/habitable-exoplanets-catalog/data/database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('phl_exoplanet_catalog.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('P_HABITABLE').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start by lumping together Probably and Possibly Habitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bindf = df.drop('P_HABITABLE', axis = 1) #This has the binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bindf['P_HABITABLE'] = (np.logical_or((df.P_HABITABLE == 1) , (df.P_HABITABLE == 2))) #turn into binary\n",
    "\n",
    "bindf['P_HABITABLE'] = bindf['P_HABITABLE'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bindf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's select some columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S_MAG - star magnitude \n",
    "\n",
    "S_DISTANCE - star distance (parsecs)\n",
    "\n",
    "S_METALLICITY - star metallicity (dex)\n",
    "\n",
    "S_MASS - star mass (solar units)\n",
    "\n",
    "S_RADIUS - star radius (solar units)\n",
    "\n",
    "S_AGE - star age (Gy)\n",
    "\n",
    "S_TEMPERATURE - star effective temperature (K)\n",
    "\n",
    "S_LOG_G - star log(g)\n",
    "\n",
    "P_DISTANCE - planet mean distance from the star (AU) \n",
    "\n",
    "P_FLUX - planet mean stellar flux (earth units)\n",
    "\n",
    "P_PERIOD - planet period (days) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going with the same features as Chapter 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = bindf[['S_MASS', 'P_PERIOD', 'P_DISTANCE']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = bindf.P_HABITABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number one rule of data science: know your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are some NaNs, e.g. shown by the \"describe\" property, which only counts numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting missing data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(final_features.shape[1]):\n",
    "    print(len(np.where(final_features.iloc[:,i].isna())[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...and getting rid of them (Note: there are better imputing strategies!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = final_features.dropna(axis = 0) #gets rid of any instance with at least one NaN in any column\n",
    "final_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1 - plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(final_features.iloc[:,0], bins = 20, alpha = 0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a remarkable outlier; The same happens for other features. But we could have also known from the difference between mean and median (which, in fact, is even more pronounced for orbital distance and period)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = final_features[(np.abs(stats.zscore(final_features)) < 5).all(axis=1)] \n",
    "\n",
    "#This eliminates > 5 sigma outliers; however it counts from the mean so it might not be ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = targets[final_features.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now reset index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = final_features.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And don't forget to do the same for the label vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = targets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the shapes, we can see that .... outliers were eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check balance of data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple way: count 0/1s, get fraction of total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(targets)/len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(targets) #this shows the distribution of the two classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also explore the data by class, to get a sense of how the two classes differ from one another. For this, we need to concatenate the feature//labels data frames so we group objects label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This generates a \"view\", not a new data frame\n",
    "\n",
    "pd.concat([final_features, targets], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can group by label and take a look at summary statistics\n",
    "\n",
    "pd.concat([final_features, targets], axis=1).groupby('P_HABITABLE').describe(percentiles = [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, this all for preliminary data exploration. Time to deploy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(final_features,targets,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.shape, Xtest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can just take a look at the train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", ['#20B2AA','#FF00FF'])\n",
    "a = plt.scatter(Xtrain['S_MASS'], Xtrain['P_PERIOD'], marker = '*',\\\n",
    "            c = ytrain, s = 100, cmap=cmap, label = 'Train')\n",
    "\n",
    "a.set_facecolor('none')\n",
    "\n",
    "a = plt.scatter(Xtest['S_MASS'], Xtest['P_PERIOD'], marker = 'o',\\\n",
    "            c = ytest, s = 100, cmap=cmap, label = 'Test')\n",
    "\n",
    "plt.legend();\n",
    "\n",
    "a.set_facecolor('none')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Mass of Parent Star (Solar Mass Units)')\n",
    "plt.ylabel('Period of Orbit (days)');\n",
    "\n",
    "bluepatch = mpatches.Patch(color='#20B2AA', label='Not Habitable')\n",
    "magentapatch = mpatches.Patch(color='#FF00FF', label='Habitable')\n",
    "\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "leg = ax.get_legend()\n",
    "leg.legendHandles[0].set_color('k')\n",
    "leg.legendHandles[1].set_color('k')\n",
    "\n",
    "\n",
    "plt.legend(handles=[leg.legendHandles[0],leg.legendHandles[1], magentapatch, bluepatch],\\\n",
    "           loc = 'lower right', fontsize = 14)\n",
    "\n",
    "#plt.savefig('LargeHPTrainTest.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "\n",
    "- Based on this graph, would you expect DT or kNN to perform better? Why?\n",
    "\n",
    "<br> \n",
    "- What kind of performance can we expect (qualitatively, is the information sufficient?) Do you expect to have latent (hidden) variables that might affect the outcome beyond those that we have?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=3)\n",
    "model.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's visualize the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder: The features are always randomly permuted at each split. \n",
    "# Therefore, the best found split may vary, even with the same training data \n",
    "# and max_features=n_features, if the improvement of the criterion is identical \n",
    "# for several splits enumerated during the search of the best split. \n",
    "# To obtain a deterministic behaviour during fitting, random_state has to be fixed.\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(\n",
    "            model,\n",
    "            out_file =  dot_data,\n",
    "            feature_names = ['Stellar Mass (M*)', 'Orbital Period (d)', 'Distance (AU)'],\n",
    "            class_names = ['Not Habitable','Habitable'],\n",
    "            filled = True,\n",
    "rounded = True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "nodes = graph.get_node_list()\n",
    "\n",
    "for node in nodes:\n",
    "    if node.get_label():\n",
    "        values = [int(ii) for ii in node.get_label().split('value = [')[1].split(']')[0].split(',')]\n",
    "        values = [255 * v / sum(values) for v in values]\n",
    "        \n",
    "        values = [int(255 * v / sum(values)) for v in values]\n",
    "            \n",
    "        if values[0] > values[1]:\n",
    "            alpha = int(values[0] - values[1])\n",
    "            alpha = '{:02x}'.format(alpha) #turn into hexadecimal\n",
    "            color = '#20 B2 AA'+str(alpha)\n",
    "        else:\n",
    "            alpha = int(values[1] - values[0])\n",
    "            alpha = '{:02x}'.format(alpha)\n",
    "            color = '#FF 00 FF'+str(alpha)\n",
    "        node.set_fillcolor(color)\n",
    "\n",
    "#graph.write_png('Graph.png',dpi = 300)\n",
    "        \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Can you predict the accuracy score on the train set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at train/test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(ytrain, model.predict(Xtrain)))\n",
    "print(metrics.accuracy_score(ytest,model.predict(Xtest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty high, but how does it compare with the accuracy of a lazy classifier that places everything in the \"not habitable\" category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy classifier\n",
    "\n",
    "print(metrics.accuracy_score(ytest,np.zeros(len(ytest))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can look at other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.precision_score(ytest,model.predict(Xtest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.recall_score(ytest,model.predict(Xtest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not perfect, but not horrible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(model.predict(Xtest)), np.sum(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(ytest,model.predict(Xtest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You know what we would need in order to understand exactly how the model is working? A confusion matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.figure(figsize=(7,6))\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\", verticalalignment=\"center\",\n",
    "                 color=\"green\" if i == j else \"red\", fontsize = 30)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can plot the confusion matrix (note that so far, we have only looked at one test fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(ytest,model.predict(Xtest))\n",
    "plot_confusion_matrix(cm, ['Not Hab','Hab'], cmap = plt.cm.Pastel2)\n",
    "#plt.savefig('CM.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three flavors of k-fold Cross Validation.\n",
    "\n",
    "Note: you can fix the random seed for exactly reproducible behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the standard version. Important: it doesn't shuffle the data, \n",
    "# so if your positive examples are all at the beginning or all the end, it might lead to disastrous results.\n",
    "\n",
    "cv1 = KFold(n_splits = 5)\n",
    "\n",
    "#This is v2: shuffling added (recommended!)\n",
    "\n",
    "cv2 = KFold(shuffle = True, n_splits = 5, random_state=5)\n",
    "\n",
    "# STRATIFICATION ensures that the class distributions in each split resembles those of the \n",
    "# entire data set (mightscores1['test_score'].mean(), scores1['test_score'].std()scores1['test_score'].mean(), scores1['test_score'].std()### Effect of stratification: let's look at the class count in each set of splits.\n",
    "\n",
    "cv3 = StratifiedKFold(shuffle = True, n_splits = 5, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of stratification: let's look at the class count in each set of splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train, test in cv1.split(final_features, targets): #Just how they are in original data set\n",
    "...     print('train -  {}   |   test -  {}'.format(\n",
    "...         np.bincount(targets.loc[train]), np.bincount(targets.loc[test])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train, test in cv2.split(final_features, targets): #One random selection\n",
    "...     print('train -  {}   |   test -  {}'.format(\n",
    "...         np.bincount(targets.loc[train]), np.bincount(targets.loc[test])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train, test in cv3.split(final_features, targets): #One adjusted-for random selection\n",
    "...     print('train -  {}   |   test -  {}'.format(\n",
    "...         np.bincount(targets.loc[train]), np.bincount(targets.loc[test])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The handy function cross\\_validate provides the scores (specified by the chosen scoring parameter), in dictionary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores1 = cross_validate(DecisionTreeClassifier(), final_features, targets, cv = cv1, scoring = 'accuracy')\n",
    "\n",
    "scores2 = cross_validate(DecisionTreeClassifier(), final_features, targets, cv = cv2, scoring = 'accuracy')\n",
    "\n",
    "scores3 = cross_validate(DecisionTreeClassifier(), final_features, targets, cv = cv3, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now calculate an average and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores1['test_score'].mean(), scores1['test_score'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2['test_score'].mean(), scores2['test_score'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores3['test_score'].mean(), scores3['test_score'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: are the differences statistically significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now use recall as our scoring parameter. Will the model change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores1 = cross_validate(DecisionTreeClassifier(), final_features, targets, cv = cv1, scoring = 'recall')\n",
    "\n",
    "scores2 = cross_validate(DecisionTreeClassifier(), final_features, targets, cv = cv2, scoring = 'recall')\n",
    "\n",
    "scores3 = cross_validate(DecisionTreeClassifier(), final_features, targets, cv = cv3, scoring = 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(scores1['test_score'].mean(), scores1['test_score'].std())\n",
    "print(scores2['test_score'].mean(), scores2['test_score'].std())\n",
    "print(scores3['test_score'].mean(), scores3['test_score'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If desired, I can ask for the train scores as well. This is very helpful when diagnosing bias vs variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores1 = cross_validate(DecisionTreeClassifier(), final_features, targets, cv = cv1, scoring = 'recall', \\\n",
    "                         return_train_score = True)\n",
    "\n",
    "scores2 = cross_validate(DecisionTreeClassifier(), final_features, targets, cv = cv2, scoring = 'recall', \\\n",
    "                         return_train_score = True)\n",
    "\n",
    "scores3 = cross_validate(DecisionTreeClassifier(), final_features, targets, cv = cv3, scoring = 'recall', \n",
    "                         return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores1['test_score'].mean(), scores1['train_score'].mean())\n",
    "print(scores2['test_score'].mean(), scores2['train_score'].mean())\n",
    "print(scores3['test_score'].mean(), scores3['train_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cross\\_validate function is useful to calculate the score, but does not produce predicted labels.\n",
    "\n",
    "#### These can be obtained by using the cross\\_val\\_predict function, which saves the predictions for each of the k test folds, and compiles them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = DecisionTreeClassifier(random_state=3)\n",
    "scores1 = cross_val_score(model1, final_features, targets, cv = cv1, scoring = 'accuracy')\n",
    "y1 = cross_val_predict(model1, final_features, targets, cv = cv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y1) #trick to see how many planets are predicted to be habitable (predicted label = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you don't believe that the model is the same, let's change the scoring parameter and train it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = DecisionTreeClassifier(random_state=3)\n",
    "scores2 = cross_val_score(model2, final_features, targets, cv = cv1, scoring = 'recall')\n",
    "y2 = cross_val_predict(model2, final_features, targets, cv = cv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y1-y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(targets,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(targets,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, things may change if I use a different cross validation scheme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = DecisionTreeClassifier(random_state=3)\n",
    "scores1 = cross_val_score(model1, final_features, targets, cv = cv1, scoring = 'accuracy')\n",
    "y1 = cross_val_predict(model1, final_features, targets, cv = cv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = DecisionTreeClassifier(random_state=3)\n",
    "scores2 = cross_val_score(model2, final_features, targets, cv = cv2, scoring = 'accuracy')\n",
    "y2 = cross_val_predict(model2, final_features, targets, cv = cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y1-y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(targets,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(targets,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = DecisionTreeClassifier(random_state=3)\n",
    "scores3 = cross_val_score(model3, final_features, targets, cv = cv3, scoring = 'accuracy')\n",
    "y3 = cross_val_predict(model3, final_features, targets, cv = cv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(targets,y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good reminder that the CM is also only one possible realization of the model, and is subject to random fluctuations just like the cross validation scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now switching to kNN classifier. Oh wait, that's for homework! :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
